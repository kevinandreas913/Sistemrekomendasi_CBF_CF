# -*- coding: utf-8 -*-
"""Proyek sistem rekomendasi - Dataset Song.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iJM7xgaEQAN-7bUj60R2-4cpxCI51nnV

# Proyek Machine Learning 2: [Song Rating Dataset]
- **Nama:** [Andreas Kevin]
- **Email:** [Kevinandreas913@gmail.com]
- **ID Dicoding:** [andreas_kevin_6396]

## Menentukan Problem Statement

- Bagaimana sistem dapat memberikan rekomendasi lagu yang relevan berdasarkan kemiripan konten (judul lagu) dan kemiripan konten tersebut tercermin dalam skor evaluasi?
- Bagaimana sistem memberikan rekomendasi lagu secara personal kepada pengguna berdasarkan favorite artis sebelumnya, dan seberapa akurat model tersebut dalam memprediksi skor/penilaian terhadap lagu yang belum pernah didengarkan?

## Import Semua Packages/Library yang Digunakan
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler

from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

import sklearn.metrics as met

from sklearn.metrics import mean_squared_error

"""## Data Understanding

### Data gathering

- Dataset diperoleh dari: https://www.kaggle.com/datasets/deependraverma13/all-songs-rating-review
- Dataset didownload dalam format .csv file
"""

df_songs = pd.read_csv("song.csv", sep=",")
display(df_songs)

"""**Insight:**
- Proses pengumpulan data berdasarkan data "All Songs Rating Review".
- Dalam menjawab pertanyaan bisnis yang disampaikan, pengumpulan data akan berfokus pada data "song.csv".
- Data dalam bentuk csv sehingga pemanggilan data dan visualisasi tabel diperlukan library pandas.

**Rincian atribut dataset:**
- Name of the song = Berisikan nama lagu-lagu.
- Artist = Berisikan nama artis dari lagu tersebut.
- Date of Release = Tanggal dirilis lagu tersebut.
- Description = Rincian deskripsi mengenai lagu.
- Metascore = Nilai rata-rata berdasarkan kritikus.
- User Score = Nilai score dari pengguna mengenai lagu tersebut.

### EDA (Exploratory Data Analysis)
"""

df_songs.info()

print("jumlah data kosong: ", df_songs.isnull().sum())
print("Jumlah duplikasi: ", df_songs.duplicated().sum())
print("Jumlah duplikasi berdasarkan judul lagu:", df_songs.duplicated(subset=['Name of the Song']).sum())

df_songs = df_songs.dropna()
df_songs = df_songs.drop_duplicates(subset=['Name of the Song'])

df_songs.isnull().sum()
df_songs.info()

"""**Insight:**
- **.info()** untuk meliht informasi atas dataset terserbut berupa jenis tipe data hingga jumlah data.
- **isnull().sum()** digunakan untuk menilai jumlah data yang kosong pada data setelah itu akan dilkaukan **.dropna()** untuk membuang baris yang kosong.
- **.duplicated().sum()** untuk menilai jumlah duplikasi pada data. Pada bagian ini terlihat jumlah duplikasi pada data adalah 0.
- **.duplicated(subset=['Name of the Song']).sum()** untuk melihat jumlah duplikasi data pada atribut Name of the Song. Pada ini terdapat data lagu yang duplicate. Maka dilkaukan **.drop_duplicates(subset=["Name of the Song"])** untuk membuang data yang kosong tersebut.
"""

df_songs_encode = df_songs.copy()
df_songs_encode['Artist'] = df_songs['Artist'].str.replace(r'^by\s+', '', regex=True)
display(df_songs_encode)

"""**Insight:**
- Pada dataframe tersebut terlihat data pada atribut **Artist** dengan setiap nama artis dimulai dengan awal "by". Maka akan dilakukan proses untuk membuang kata by di setiap awal nama artis tersebut.
"""

def histogram(name):
    plt.figure(figsize=(10, 6))
    sns.histplot(df_songs_encode[name], bins=30, kde=True)
    plt.xlabel(name)
    plt.ylabel("Frequency")
    plt.title(f"Histogram untuk {name}")
    plt.show()

for col in df_songs.select_dtypes(include=[np.number]).columns:
    print(f"Histogram untuk {col}:")
    histogram(col)

"""**Insight:**
- Visualisasi histogram bertujuan untuk melihat pernyebaran data saat ini.

## Data Prepration Content Base Filtering
"""

df_songs = df_songs_encode.copy()
df_songs= df_songs.drop(['Description', 'Unnamed: 0', 'Date of Release'], axis = 1)
display(df_songs)

len(df_songs['Name of the Song'].unique())

df_songs.info()

"""**Insight:**
- Proses Data preparation untuk Content Base Filetering dilakukan dengan membuang atribut **Description, Unnamed: 0, dan Date of Release**. Tujuan dari pembuangan atribut ini untuk mempersiapkan atribut yang akan digunakan pada proses CBF.
- Jumlah total data yang digunakan pada proses ini adalah 2537 data dengan atribut yang digunakan berupa nama lagu, artis, metascore, dn user score.

## Model Development Content Base Filtering
"""

tf = TfidfVectorizer()

tfidf_matrix = tf.fit_transform(df_songs['Name of the Song'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

tfidf_matrix.todense()

"""**Insight:**
- **TfidfVectorizer()** digunakan untuk mengubah teks menjadi representasi numerik berbasis TF-IDF (Term Frequency-Inverse Document Frequency), yang mencerminkan seberapa penting suatu kata dalam dokumen.
- **fit_transform(df_songs['Name of the Song'])** melakukan proses pelatihan dan transformasi langsung terhadap data lagu menjadi matriks TF-IDF.
- **tfidf_matrix.shape** digunakan untuk melihat ukuran matriks TF-IDF, menunjukkan jumlah lagu (baris) dan jumlah fitur unik (kolom) dari judul lagu.
- **tfidf_matrix.todense()** digunakan untuk mengubah matriks sparse menjadi bentuk dense agar lebih mudah dibaca dan dianalisis secara langsung.
"""

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=df_songs['Artist']
).sample(10, axis=1).sample(10, axis=0)

"""**Insight:**
- Proses ini dilkaukan untuk menyesuaikan dataframe dengan menjadikan Artist sebagai index.
"""

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

"""**Insight:**
- Proses cosine_similarity digunakan untuk menhitung similarity dari matriks yang telah dibuat.
"""

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama lagu
cosine_sim_df = pd.DataFrame(cosine_sim, index=df_songs['Name of the Song'], columns=df_songs['Name of the Song'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap resto
cosine_sim_df.sample(10, axis=0).sample(10, axis=0)

def song_recomendations(target_song_artist, similarity_data=cosine_sim_df, items=df_songs[['Name of the Song', 'Artist']], k=3):
    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,target_song_artist].to_numpy().argpartition(
        range(-1, -k, -1)).flatten()

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop target_song_artist agar nama artis yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(target_song_artist, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

"""**Insight:**
- **def song_recommendations(...)** digunakan sebagai fungsi rekomendasi lagu berdasarkan kemiripan (similarity) antar lagu dan artis.
- **similarity_data.loc[:, target_song_artist].to_numpy().argpartition(...)** digunakan untuk mencari index dari lagu yang paling mirip (nilai similarity tertinggi) terhadap lagu/artis target, tanpa mengurutkan seluruh data (lebih efisien).
- **closest = similarity_data.columns[index[-1:-(k+2):-1]]** mengambil k+1 lagu teratas dari hasil similarity tertinggi (termasuk lagu target).
- **closest.drop(target_song_artist, errors='ignore')** memastikan lagu/artis yang sama tidak direkomendasikan.
- **pd.DataFrame(closest).merge(items)** menggabungkan hasil rekomendasi dengan informasi lagu dan artis dari data asli.
"""

song_recomendations('Dry Food')

song_recomendations('Robyn')

song_recomendations('Hard Candy')

"""**Insight:**
- Ketiga proses tersebut merupakan contoh penggunakan fungsi dari **song_recomendations** yang telah dibuat sebelumnya dalam memberikan 3 lagu terbaik.

## Evaluation Content Base Filtering
"""

def avg_cosine_similarity(k=3):
    total_sim = 0
    count = 0

    for idx, row in df_songs.iterrows():
        target_song = row['Name of the Song']

        if target_song not in cosine_sim_df.columns:
            continue
        try:
            recs = song_recomendations(target_song, k=k)
        except:
            continue

        # Ambil similarity score antara target dan rekomendasi
        for rec_song in recs['Name of the Song']:
            sim_score = cosine_sim_df.loc[target_song, rec_song]
            total_sim += sim_score
            count += 1

    avg_sim = total_sim / count if count else 0
    print(f"Avg Cosine Similarity@{k}: {avg_sim:.4f}")
    return avg_sim

avg_cosine_similarity(3)

"""**Insight:**  
**Average cousin similarity merupakan teknik yang dibuat dengan tujuan untuk evaluasi model Content Base Filtering. Teknik evaluasi ini berbeda dengan teknik evaluasi berdasarkan precision recall, f1-score yang telah disahkan. Teknik ini hanya digunakan untuk validasi internal atas model yang telah dibangun.**
</br>  

- **def avg_cosine_similarity(k=3):** merupakan fungsi untuk menghitung rata-rata nilai kemiripan (cosine similarity) antara lagu target dan hasil rekomendasinya.
- **for idx, row in df_songs.iterrows():** digunakan untuk melakukan iterasi ke setiap lagu dalam dataset.
- **if target_song not in cosine_sim_df.columns:** mengecek apakah lagu target ada di dalam data similarity, jika tidak maka dilewati.
- **song_recommendations(target_song, k=k)** dipanggil untuk mendapatkan rekomendasi lagu berdasarkan lagu target.
- **cosine_sim_df.loc[target_song, rec_song]** mengambil nilai cosine similarity antara lagu target dan lagu hasil rekomendasi.
- **total_sim += sim_score** menjumlahkan semua nilai similarity dari seluruh lagu dan rekomendasinya.
- **avg_sim = total_sim / count if count else 0** menghitung rata-rata similarity hanya jika ada data yang valid, jika tidak maka hasilnya 0.
</br>  

**Berdasarkan hasil average cosine similarity, maka untuk hasil 0,29 dapat dikatakan cukup mirip dengan konten, hal ini dikarenakan dataset yang sangat besar dan beragam. Selain itu ini mengukur kemiripan (similarity) berdasarkan judul lagu sehingga hasil teresbut dapat dikatakan sebagai baik.**

## Data Preparation Collaborative Filtering
"""

display(df_songs)

le_artist = LabelEncoder()
le_song = LabelEncoder()

df_songs['Artist'] = le_artist.fit_transform(df_songs['Artist']) + 1
df_songs['Name of the Song'] = le_song.fit_transform(df_songs['Name of the Song']) + 1
df_songs.drop(['User Score'], axis=1)
display(df_songs)

for i in df_songs.columns:
    print(f"data {i}",df_songs[i].nunique())
    print("-" * 50)

# Nilai minimum rating
min_rating = min(df_songs['Metascore'])

# Nilai maksimal rating
max_rating = max(df_songs['Metascore'])

# Membuat variabel x untuk mencocokkan data user dan resto menjadi satu value
x = df_songs[['Artist', 'Name of the Song']].values

# Membuat variabel y untuk membuat rating dari hasil
y = df_songs['Metascore'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

scaler_x = MinMaxScaler()
x = scaler_x.fit_transform(x)

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df_songs.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""**Insight:**  
Proses data preparation untuk collaborative filtering meliputi:  
- Melakaukan encoder atas atribut **Artist** dan **Name of the Song** menjadi numerik.
- **print(f"data {i}",df_songs[i].nunique())** memastikan jumlah data yang bersifat unique.
- Melakukan normaliasi atas data dan membagi data menjadi train dan test dengan variabel X dan y.

## Model Development Collaborative Filtering
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

class RecommenderNet(tf.keras.Model):
    def __init__(self, num_artists, num_songs, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.num_artists = num_artists
        self.num_songs = num_songs
        self.embedding_size = embedding_size
        self.dropout = layers.Dropout(0.3)

        # Layer embedding untuk artist
        self.artist_embedding = layers.Embedding(
            input_dim=num_artists,
            output_dim=embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-4)
        )
        self.artist_bias = layers.Embedding(num_artists, 1)

        # Layer embedding untuk song
        self.song_embedding = layers.Embedding(
            input_dim=num_songs,
            output_dim=embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-4)
        )
        self.song_bias = layers.Embedding(num_songs, 1)

    def call(self, inputs):
        artist_vector = self.artist_embedding(inputs[:, 0])
        artist_bias = self.artist_bias(inputs[:, 0])

        song_vector = self.song_embedding(inputs[:, 1])
        song_bias = self.song_bias(inputs[:, 1])

        # Dot product antara artist dan song
        dot_artist_song = tf.reduce_sum(artist_vector * song_vector, axis=1, keepdims=True)

        # Menambahkan bias
        x = dot_artist_song + artist_bias + song_bias
        x = self.dropout(x)

        # Aktivasi sigmoid untuk output antara 0 dan 1
        return tf.nn.sigmoid(x)

"""**Insight:**
- **class RecommenderNet(tf.keras.Model):** mendefinisikan model neural network kustom untuk sistem rekomendasi
- **song_embedding** dan **artis embedding** membuat representasi vektor berdimensi embedding_size untuk setiap artist dan lagu, yang dilatih agar bisa menangkap hubungan antar entitas.
- **artist_bias** dan **song_bias** digunakan untuk menambahkan bias khusus pada masing-masing entitas, meningkatkan akurasi prediksi.
- **dot_artist_song = tf.reduce_sum(artist_vector * song_vector, axis=1, keepdims=True)** mengalikan antar vektor embedding artist dan song, sebagai representasi interaksi mereka.
- **x = dot_artist_song + artist_bias + song_bias** menambahkan bias ke hasil dot product untuk mendapatkan prediksi akhir.
- **self.dropout(x)** menerapkan dropout untuk mencegah overfitting dengan mengabaikan sebagian neuron secara acak saat pelatihan.
- **return tf.nn.sigmoid(x)** menggunakan aktivasi sigmoid agar output berada pada rentang 0–1.
"""

num_artists = df_songs['Artist'].nunique()
num_songs = df_songs['Name of the Song'].nunique()

model = RecommenderNet(num_artists + 1, num_songs + 1, 50)

# Compile model
# model.compile(
#     loss=tf.keras.losses.BinaryCrossentropy(),  # karena output sigmoid (0-1)
#     optimizer=keras.optimizers.Adam(learning_rate=0.001),
#     metrics=[tf.keras.metrics.RootMeanSquaredError()]
# )

model.compile(
    loss=tf.keras.losses.MeanSquaredError(),  # karena output sigmoid (0-1)
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""**Insight:**
- **num_artists = df_songs['Artist'].nunique()** dan **num_songs = df_songs['Name of the Song'].nunique()** digunakan untuk menghitung jumlah unik artis dan lagu sebagai input ke layer embedding.
- **model = RecommenderNet(num_artists + 1, num_songs + 1, 50)** membuat model dengan dimensi embedding sebesar 50, dan ditambah 1 untuk menghindari index error pada layer embedding.
- **model.compile(...)** digunakan untuk mengonfigurasi model sebelum pelatihan.
- **loss=tf.keras.losses.MeanSquaredError()** dipilih karena output model berupa nilai kontinu (0–1) dari sigmoid.
- **optimizer=keras.optimizers.Adam(learning_Rate=0.001)** untuk mempercepat konvergensi dan efisiensi pelatihan dengan banuan learning rate.
- **metrics=[tf.keras.metrics.RootMeanSquaredError()]** untuk memantau seberapa dekat prediksi terhadap target selama pelatihan.
"""

class myCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        rmse = logs.get('root_mean_squared_error')
        if rmse is not None and rmse < 0.05:
            print(f"\nRMSE telah mencapai < 0.05! (RMSE: {rmse:.4f})")
            self.model.stop_training = True
callbacks = myCallback()

early_stop = tf.keras.callbacks.EarlyStopping(
    monitor='val_root_mean_squared_error',
    patience=10,
    restore_best_weights=True,
    verbose=1
)

reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    min_lr=1e-6,
    verbose=1
)

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 64,
    epochs = 100,
    callbacks=[callbacks, early_stop, reduce_lr],
    validation_data = (x_val, y_val),
    verbose=1
)

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""**Insight:**
- **class myCallback(tf.keras.callbacks.Callback):** digunakan untuk membuat calback sehingga menghentikan proses training jika mencapai konvergensi yang diinginkan.
- **early_stop = tf.keras.callbacks.EarlyStopping(...)** digunakan untuk menghentikan pelatihan jika performa validasi tidak membaik selama 10 epoch berturut-turut, mencegah overfitting.
- **reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(...)** akan mengurangi learning rate sebesar 50% jika val_loss tidak membaik selama 5 epoch, membantu model keluar dari plateau.
- **model.fit(...)** menjalankan proses pelatihan model dengan data training dan validasi, serta memanfaatkan tiga callback sekaligus untuk mengontrol pelatihan secara dinamis dan efisien.
</br>

Untuk membaca hasil training dapat dilakukan sebagai berikut:
- **loss:** Loss pada data training untuk menilai besar kesalahan model saat melatih data.
- **root_mean_squared_error:** RMSE pada data training untuk menilai rata-rata kesalahan model saat training.
- **val_loss:** Loss pada data validasi untuk menilai besar kesalahan model saat diuji ke data baru (tidak dilatih).
- **val_root_mean_squared_error:** RMSE pada data validasi untuk menilai rata-rata kesalahan pada data validasi.
"""

# Ambil sample artis secara acak
artist_id = df_songs.Artist.sample(1).iloc[0]

# Lagu yang sudah pernah dibuat oleh artis itu
songs_by_artist = df_songs[df_songs.Artist == artist_id]

# Lagu yang belum dibuat oleh artis tsb (untuk rekomendasi)
songs_not_by_artist = df_songs[~df_songs['Name of the Song'].isin(songs_by_artist['Name of the Song'].values)]

# Ambil hanya kolom 'Name of the Song'
songs_not_by_artist_ids = songs_not_by_artist['Name of the Song'].values

# Buat array pasangan [artist_id, song_id] untuk semua lagu yang belum dibuat oleh artis itu
artist_song_array = np.hstack(
    ([[artist_id]] * len(songs_not_by_artist_ids), songs_not_by_artist_ids.reshape(-1, 1))
)

# Lakukan prediksi rating
ratings = model.predict(artist_song_array).flatten()

# Ambil 10 rekomendasi teratas
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_song_ids = songs_not_by_artist_ids[top_ratings_indices]

# Inverse transform untuk menampilkan nama asli
artist_name = le_artist.inverse_transform([artist_id - 1])[0]  # Karena sebelumnya +1 waktu encode
recommended_song_titles = le_song.inverse_transform(recommended_song_ids.astype(int))

# Tampilkan hasil
print(f'Rekomendasi lagu untuk artist: {artist_name} (ID: {artist_id})')
print('=' * 30)

print('Lagu-lagu oleh artist ini:')
print('-' * 30)
top_artist_songs = songs_by_artist.sort_values(by='Metascore', ascending=False).head(5)
for idx, row in top_artist_songs.iterrows():
    song_title = le_song.inverse_transform([int(row['Name of the Song']) - 1])[0]  # Karena sebelumnya +1
    print(f"{song_title} (Metascore: {row['Metascore']})")

print('-' * 30)
print('Top 10 Lagu Rekomendasi:')
print('-' * 30)
for i, song_id in enumerate(recommended_song_ids):
    song_title = recommended_song_titles[i]
    print(f"Lagu: {song_title}")

"""**Insight:**
- **artist_id = df_songs.Artist.sample(1).iloc[0]** digunakan untuk mengambil artis secara acak dari dataset.
- **songs_by_artist = df_songs[df_songs.Artist == artist_id]** berisi semua lagu yang sudah pernah dibuat oleh artis tersebut.
- **songs_not_by_artist = df_songs[~df_songs['Name of the Song'].isin(songs_by_artist['Name of the Song'].values)]** berisi lagu-lagu yang belum dibuat oleh artis itu, dan akan menjadi kandidat untuk rekomendasi.
- **artist_song_array** array berisi pasangan [artist_id, song_id] yang dibentuk untuk setiap lagu yang belum dibuat oleh artis, sebagai input ke model.
- **model.predict(...)** untuk memprediksi seberapa besar kemungkinan artis menyukai atau membuat lagu-lagu tersebut.
- **ratings.argsort()[-10:][::-1]** mengambil 10 lagu teratas dengan skor prediksi tertinggi sebagai hasil rekomendasi.
- **le_artist.inverse_transform(...)** dan **le_song.inverse_transform(...)** digunakan untuk mengubah kembali ID artis dan lagu ke bentuk nama aslinya, karena sebelumnya sudah melalui proses encoding.
- **print(...)** menampilkan informasi artis, lagu-lagu yang pernah dibuatnya, dan 10 lagu rekomendasi berdasarkan hasil prediksi model.

## Evaluation Collaborative Filtering
"""

y_pred = model.predict(x_val).flatten()

# RMSE (Root Mean Squared Error)
rmse = np.sqrt(mean_squared_error(y_val, y_pred))
print(f"RMSE pada validation set: {rmse:.4f}")

"""**Insight:**
- Evaluasi digunakan dengan melakukan prediksi yang disimpan pada variabel y_pred dan prediksi dilkaukan dengan menilai RMSE.
- RMSE digunakan sebagai teknik untuk evaluasi dikarenakan ini cocok untuk kasus collaborative filtering dalam mengukur jarak hasil asli dengan hasil prediksi.
- Nilai RMSE (0.1946) dapat dikatakan baik dikarenakan RMSE < 0,2. Hal ini dengan mempertimbangkan dataset yang besar dan bervariasi sehingga Collaborative Filtering dianggap mampu untuk menebak score lagu.
"""